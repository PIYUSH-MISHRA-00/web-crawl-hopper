# Web Crawl Hopper ğŸ•·ï¸

A sophisticated web crawler with a real-time interactive dashboard, built using Node.js, Express, and Socket.IO. This project features a modern web interface for controlling and monitoring the crawling process, with support for depth-limited crawling, rate limiting, and intelligent URL parsing.

## Features ğŸŒŸ

### Core Functionality
- Interactive web dashboard for real-time crawl monitoring
- Depth-limited crawling with configurable maximum depth
- Domain-scoped crawling (stays within the initial domain)
- Robots.txt compliance
- Intelligent URL parsing and normalization
- Product URL detection

### Real-time Controls
- Start/Stop crawling
- Pause/Resume functionality
- Live progress monitoring
- Real-time crawl statistics
- URL filtering capabilities

### Advanced Features
- Rotating User-Agent headers to prevent blocking
- Intelligent rate limiting with random delays
- Automatic retry mechanism for failed requests
- Error handling and detailed logging
- Concurrent request handling

### User Interface
- Clean, modern dashboard design using Tailwind CSS
- Real-time activity log with timestamps
- Filterable URL list
- Live statistics display
- Responsive design for all screen sizes

## Technology Stack ğŸ’»

- **Backend**
  - Node.js
  - Express.js
  - Socket.IO for real-time updates
  - JSDOM for HTML parsing
  - Axios for HTTP requests

- **Frontend**
  - HTML5
  - Tailwind CSS
  - Socket.IO client
  - Vanilla JavaScript

## Prerequisites ğŸ“‹

- Node.js (v14 or higher)
- npm (Node Package Manager)

## Installation ğŸš€

1. Clone the repository:
```bash
git clone https://github.com/PIYUSH-MISHRA-00/web-crawl-hopper.git
cd web-crawl-hopper
```

2. Install dependencies:
```bash
npm install
```

3. Start the server:
```bash
npm run dev
```

4. Open your browser and navigate to `http://localhost:3000`

## Usage Guide ğŸ“–

### Starting a Crawl
1. Enter the target URL in the input field
2. Set the maximum crawl depth (1-10)
3. Click "Start Crawling"

### Control Options
- **Pause**: Temporarily halt the crawling process
- **Resume**: Continue a paused crawl
- **Stop**: Completely stop the crawling process

### Monitoring
- View real-time statistics in the dashboard
- Monitor the activity log for detailed progress
- Filter and search through discovered URLs
- Track successful and failed crawls

## Project Structure ğŸ“

```
web-crawl-hopper/
â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ index.html      # Web interface
â”‚   â””â”€â”€ styles.css      # Tailwind CSS styles
â”œâ”€â”€ crawl.js           # Core crawler logic
â”œâ”€â”€ server.js          # Express server & Socket.IO setup
â”œâ”€â”€ package.json       # Project dependencies
â””â”€â”€ README.md          # Documentation
```

## Key Components âš™ï¸

### Crawler (\`crawl.js\`)
- Manages the crawling process
- Handles URL queuing and processing
- Implements rate limiting and retries
- Processes robots.txt
- Manages state (running/paused/stopped)

### Server (\`server.js\`)
- Serves the web interface
- Handles API endpoints
- Manages Socket.IO connections
- Broadcasts crawl updates

### Web Interface (\`public/index.html\`)
- Real-time dashboard
- Control panel
- Statistics display
- Activity log
- URL list with filtering

## Error Handling ğŸ› ï¸

- Automatic retry for failed requests
- Rate limit detection and handling
- Invalid URL filtering
- Detailed error logging
- Graceful error recovery

## Performance Considerations ğŸ“Š

- Configurable crawl depth
- Random delays between requests
- Concurrent request limiting
- Memory-efficient URL tracking
- Optimized DOM parsing

## Author âœï¸

PIYUSH-MISHRA-00

## Acknowledgments ğŸ™

- Socket.IO for real-time capabilities
- Tailwind CSS for the UI design
- JSDOM for HTML parsing
- Axios for HTTP requests

---

Made with â¤ï¸ by PIYUSH-MISHRA-00